#cloud-config

hostname: {{ coreos_hostname }}
ssh_authorized_keys:
  {% for key in coreos_public_keys %}
  - {{ key }}
  {% endfor %}

write_files:
  - path: /opt/bin/ceph
    permissions: '0775'
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -i --net=host -v /etc/ceph:/etc/ceph ceph/base ceph "$@"
  - path: /opt/bin/rbd
    permissions: '0775'
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -i --net=host -v /dev:/dev -v /sys:/sys --net=host --privileged=true -v /etc/ceph:/etc/ceph ceph/base rbd "$@"
  - path: /opt/bin/rados
    permissions: '0775'
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -i  --net=host -v  /etc/ceph:/etc/ceph ceph/base rados "$@"
  - path: /opt/bin/rados
    permissions: '0775'
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -i  --net=host -v /etc/ceph:/etc/ceph ceph/base rados "$@"
  - path: /opt/bin/cleanup
    permissions: '0775'
    content: |
      #!/bin/sh
      /usr/bin/rm -rf /etc/systemd/system/*
      /usr/bin/rm -rf /etc/kubernetes/manifests
      /usr/bin/echo "cleanup done"

  - path: /opt/bin/check-cordon
    permissions: '0775'
    content: |
      #!/bin/bash
      /opt/bin/wupiao localhost:8888
      /usr/bin/docker run --rm --net=host -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ quay.io/cornelius/hyperkube:v1.3.5_coreos.1 \
      /hyperkube kubectl --client-key=/etc/kubernetes/ssl/worker-key.pem --client-certificate=/etc/kubernetes/ssl/worker.pem \
      --server=https://127.0.0.1:8888 --certificate-authority=/etc/kubernetes/ssl/ca.pem get nodes  | grep {{ inventory_hostname }} | grep  -q SchedulingDisabled 
  - path: /opt/bin/uncordon-node
    permissions: '0775'
    content: |
      #!/bin/sh
      while /opt/bin/check-cordon
      do
         /usr/bin/docker run --rm --net=host -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ quay.io/cornelius/hyperkube:v1.3.5_coreos.1 \
         /hyperkube kubectl --client-key=/etc/kubernetes/ssl/worker-key.pem --client-certificate=/etc/kubernetes/ssl/worker.pem \
         --server=https://127.0.0.1:8888 --certificate-authority=/etc/kubernetes/ssl/ca.pem uncordon {{ inventory_hostname }} || true
      done

  - path: /opt/bin/drain-node
    permissions: '0775'
    content: |
      #!/bin/bash

      ERR=1 # or some non zero error number you want
      MAX_TRIES=4 
      COUNT=0
      while [  $COUNT -lt $MAX_TRIES ]; do
         /usr/bin/docker run --rm --net=host -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ quay.io/cornelius/hyperkube:v1.3.5_coreos.1 /hyperkube kubectl --client-key=/etc/kubernetes/ssl/worker-key.pem --client-certificate=/etc/kubernetes/ssl/worker.pem --server=https://127.0.0.1:8888 --certificate-authority=/etc/kubernetes/ssl/ca.pem drain {{ inventory_hostname }} --ignore-daemonsets=true --force --delete-local-data
         if [ $? -eq 0 ];then
            # try to unlock all rbd images
            echo "unlocking rbd images held by this node"

            #for image in $(/opt/bin/rbd ls); do
            #    echo "listing locks for $image"
            #    /opt/bin/rbd lock list $image | grep $(/bin/hostname) | while read lock; do
            #        echo "reoving lock $lock"
            #        /opt/bin/rbd lock rm $image $(echo $lock | awk '{print $2}') $(echo $lock | awk '{print $1}')
            #        # skipping this because it seems to lead to deadlocks if kubelet manages to get to the point to delete it too
            #    done 
            #done

            sleep 30 # give pods some time to evict
            exit 0
         fi
         let COUNT=COUNT+1
      done
      echo "Too many non-successful tries"
      exit $ERR

  - path: /opt/bin/hyperkube-wrapper
    permissions: '0775'
    content: |
      #!/bin/bash
      # Wrapper for launching kubelet via rkt-fly stage1. 
      #
      # Make sure to set KUBELET_VERSION to an image tag published here:
      # https://quay.io/repository/coreos/hyperkube?tab=tags Alternatively,
      # override $KUBELET_ACI to a custom location.
      
      set -e
      
      if [ -z "${KUBELET_VERSION}" ]; then
          echo "ERROR: must set KUBELET_VERSION"
          exit 1
      fi
      
      KUBELET_ACI="${KUBELET_ACI:-quay.io/coreos/hyperkube}"
      
      mkdir --parents /etc/kubernetes
      mkdir --parents /var/lib/docker
      mkdir --parents /var/lib/kubelet
      mkdir --parents /run/kubelet
      
      exec /usr/bin/rkt run \
        --volume etcd-certs,kind=host,source=/etc/ssl/etcd/ \
        --volume etc-kubernetes,kind=host,source=/etc/kubernetes \
        --volume etc-ssl-certs,kind=host,source=/usr/share/ca-certificates \
        --volume var-lib-docker,kind=host,source=/var/lib/docker \
        --volume var-lib-kubelet,kind=host,source=/var/lib/kubelet \
        --volume os-release,kind=host,source=/usr/lib/os-release \
        --volume run,kind=host,source=/run \
        --mount volume=etcd-certs,target=/etc/ssl/etcd/ \
        --mount volume=etc-kubernetes,target=/etc/kubernetes \
        --mount volume=etc-ssl-certs,target=/etc/ssl/certs \
        --mount volume=var-lib-docker,target=/var/lib/docker \
        --mount volume=var-lib-kubelet,target=/var/lib/kubelet \
        --mount volume=os-release,target=/etc/os-release \
        --mount volume=run,target=/run \
        --trust-keys-from-https \
        $RKT_OPTS \
        --stage1-from-dir=stage1-fly.aci \
        ${KUBELET_ACI}:${KUBELET_VERSION} --exec=/${EXEC} -- "$@"

  {%if inventory_hostname in groups['kubernetes-master'] or  inventory_hostname in groups['kubernetes-node'] %}

  - path: /etc/kubernetes/ssl/ca.pem
    content: |
      {{ kube_ca|indent(width=6) }}

  {% endif %}
  {%if inventory_hostname in groups['kubernetes-master'] %}


  - path: /etc/kubernetes/ssl/ca-key.pem
    content: |
      {{ kube_ca_key|indent(width=6) }}

  - path: /etc/conf.d/nfs
    permissions: '0644'
    content: |
      OPTS_RPC_MOUNTD=""
  - path: /etc/kubernetes/ssl/apiserver.pem
    content: |
      {{ kube_apiserver_pem|indent(width=6) }}
  - path: /etc/kubernetes/ssl/apiserver-key.pem
    content: |
      {{ kube_apiserver_key|indent(width=6) }}

#  # kube api server
#  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
#    content: |
#      apiVersion: v1
#      kind: Pod
#      metadata:
#        name: kube-apiserver
#        namespace: kube-system
#      spec:
#        hostNetwork: true
#        containers:
#        - name: kube-apiserver
#          image: {{ hyperkube_aci }}:{{ hyperkube_aci_tag }}
#          command:
#          - /hyperkube
#          - apiserver
#          - --bind-address=0.0.0.0
#          - --etcd-servers=https://{{ inventory_hostname }}:2379
#          - --etcd-cafile=/etc/ssl/etcd/ca.crt
#          - --etcd-certfile=/etc/ssl/etcd/key.crt
#          - --etcd-keyfile=/etc/ssl/etcd/key.key
#          - --allow-privileged=true
#          - --service-cluster-ip-range={{ k8s_service_ip_range }}
#          - --secure-port=6443
#          - --advertise-address={{ inventory_hostname }}
#          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
#          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
#          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
#          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
#          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
#          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/thirdpartyresources=true
#          ports:
#          - containerPort: 6443
#            hostPort: 6443
#            name: https
#          - containerPort: 8080
#            hostPort: 8080
#            name: local
#          volumeMounts:
#          - mountPath: /etc/kubernetes/ssl
#            name: ssl-certs-kubernetes
#            readOnly: true
#          - mountPath: /etc/ssl/certs
#            name: ssl-certs-host
#            readOnly: true
#          - mountPath: /etc/ssl/etcd
#            name: ssl-certs-etcd
#            readOnly: true
#        volumes:
#        - hostPath:
#            path: /etc/kubernetes/ssl
#          name: ssl-certs-kubernetes
#        - hostPath:
#            path: /etc/ssl/etcd
#          name: ssl-certs-etcd
#        - hostPath:
#            path: /usr/share/ca-certificates
#          name: ssl-certs-host
#  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
#    content: |
#      apiVersion: v1
#      kind: Pod
#      metadata:
#        name: kube-controller-manager
#        namespace: kube-system
#      spec:
#        hostNetwork: true
#        containers:
#        - name: kube-controller-manager
#          image: {{ hyperkube_aci }}:{{ hyperkube_aci_tag }}
#          command:
#          - /hyperkube
#          - controller-manager
#          - --master=http://127.0.0.1:8080
#          - --leader-elect=true
#          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
#          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
#          - --pod-eviction-timeout=10s
#          - --node-monitor-grace-period=10s
#          livenessProbe:
#            httpGet:
#              host: 127.0.0.1
#              path: /healthz
#              port: 10252
#            initialDelaySeconds: 15
#            timeoutSeconds: 1
#          volumeMounts:
#          - mountPath: /etc/kubernetes/ssl
#            name: ssl-certs-kubernetes
#            readOnly: true
#          - mountPath: /etc/ssl/certs
#            name: ssl-certs-host
#            readOnly: true
#        volumes:
#        - hostPath:
#            path: /etc/kubernetes/ssl
#          name: ssl-certs-kubernetes
#        - hostPath:
#            path: /usr/share/ca-certificates
#          name: ssl-certs-host
#  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
#    content: |
#      apiVersion: v1
#      kind: Pod
#      metadata:
#        name: kube-scheduler
#        namespace: kube-system
#      spec:
#        hostNetwork: true
#        containers:
#        - name: kube-scheduler
#          image: {{ hyperkube_aci }}:{{ hyperkube_aci_tag }}
#          command:
#          - /hyperkube
#          - scheduler
#          - --master=http://127.0.0.1:8080
#          - --leader-elect=true
#          livenessProbe:
#            httpGet:
#              host: 127.0.0.1
#              path: /healthz
#              port: 10251
#            initialDelaySeconds: 15
#            timeoutSeconds: 1



  {% endif %}

  {% if extra_cas.results is defined %}
  {% for item in extra_cas.results %}

  - path: /etc/ssl/certs/{{ item.item|basename }}
    encoding: base64
    content: |
      {{ item.content|indent(width=6) }}

  {% endfor %}
  {% endif %}

  {%if inventory_hostname in groups['kubernetes-node'] or inventory_hostname in groups['kubernetes-master'] %}

#  - path: /etc/kubernetes/manifests/kube-proxy.yaml
#    content: |
#      apiVersion: extensions/v1beta1
#      kind: DaemonSet
#      metadata:
#        name: kube-proxy-{{ inventory_hostname }}
#        namespace: kube-system
#        labels:
#          app: kube-proxy
#      spec:
#        template:
#          metadata:
#            labels:
#              app: kube-proxy
#          spec:
#            hostNetwork: true
#            nodeName: "{{ inventory_hostname }}"
#            containers:
#            - name: kube-proxy
#              image: quay.io/cornelius/hyperkube:v1.3.5_coreos.1
#              command:
#              - /hyperkube
#              - proxy
#              - --master=https://127.0.0.1:8888 
#              - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
#              - --proxy-mode=iptables
#              securityContext:
#                privileged: true
#              volumeMounts:
#                - mountPath: /etc/ssl/certs
#                  name: "ssl-certs"
#                - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
#                  name: "kubeconfig"
#                  readOnly: true
#                - mountPath: /etc/kubernetes/ssl
#                  name: "etc-kube-ssl"
#                  readOnly: true
#            volumes:
#              - name: "ssl-certs"
#                hostPath:
#                  path: "/usr/share/ca-certificates"
#              - name: "kubeconfig"
#                hostPath:
#                  path: "/etc/kubernetes/worker-kubeconfig.yaml"
#              - name: "etc-kube-ssl"
#                hostPath:
#                  path: "/etc/kubernetes/ssl"

  - path: /etc/kubernetes/worker-kubeconfig.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context

  - path: /etc/kubernetes/ssl/worker.pem
    content: |
      {{ kube_worker_pem|indent(width=6) }}
  - path: /etc/kubernetes/ssl/worker-key.pem
    content: |
      {{ kube_worker_key|indent(width=6) }}
  {% endif %}

  - path: /opt/bin/update-window.sh
    permissions: 0755
    owner: root
    content: |
      #!/bin/bash
      # sleep a rondom delay between 0 and five minutes to prevent every machine trying to get the lock at the same time
      delay=$(/usr/bin/expr $RANDOM % 300 )
      sleep $delay
      
      if locksmithctl lock || locksmithctl status | grep `cat /etc/machine-id`; then
        rebootflag='NEED_REBOOT'
        if update_engine_client -status | grep $rebootflag || etcdctl get  /needsreboot/$( cat /etc/machine-id); then
          if /opt/bin/ceph  health | grep HEALTH_OK && /opt/bin/ceph -s | grep "{{ groups['ceph-mon'] | length }} mons"; then
            etcdctl rm /needsreboot/$( cat /etc/machine-id) || true;
            echo "all checks for update window passed; rebooting now";
            reboot;
          else
            echo "reboot needed but ceph is not health; delay reboot"
            locksmithctl unlock;
          fi
        else
            echo "no reboot needed, unlock"
            locksmithctl unlock;
        fi
      fi
  - path: /opt/bin/update_needed.sh
    permissions: 0755
    owner: root
    content: |
      #!/bin/bash
      . /etc/environment
      etcdctl set /needsreboot/$( cat /etc/machine-id) $( cat /etc/machine-id)

  - path: /opt/bin/waiter.sh
    owner: root
    content: |
      #! /usr/bin/bash
      until curl --cacert /etc/ssl/etcd/ca.crt --cert /etc/ssl/etcd/key.crt --key /etc/ssl/etcd/key.key https://{{ inventory_hostname }}:4001/v2/machines -v; do sleep 2; done


  - path: /etc/etcd-client.config.json
    permissions: '0644'
    content: |
       { 
         "cluster": {
         "machines": [ "https://{{ inventory_hostname }}:2379" ] },
         "config": { 
            "certFile": "/etc/ssl/etcd/key.crt",
            "keyFile": "/etc/ssl/etcd/key.key",
            "caCertFiles": [ "/etc/ssl/etcd/ca.crt" ],
         "timeout": 5000000000,
         "consistency": "WEAK"
         } 
       }


  - path: /opt/bin/wupiao
    permissions: '0755'
    content: |
      #!/bin/bash
      # [w]ait [u]ntil [p]ort [i]s [a]ctually [o]pen
      [ -n "$1" ] && \
        until curl -o /dev/null -sIf http://${1}; do \
          sleep 1 && echo .;
        done;
      exit $?
  - path: /srv/tinc_initial_config.sh
    permissions: 0774
    owner: root
    content: |
      #!/bin/sh
      export DOCKER_HOST=unix:///var/run/early-docker.sock
      . /etc/tinc-env
      for host in `etcdctl ls /services/tinc/ | sed -e 's/\/services\/tinc\///'`; do
        if [ "$TINC_HOSTNAME" != "$host" ]; then
          docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add ConnectTo = $host
          etcdctl get /services/tinc/$host | sed -e 's/\"//g' > /srv/tinc/hosts/$host
        fi
      done
      docker exec tinc /usr/sbin/tinc reload
  - path: /etc/environment
    permissions: 0774
    owner: root
    content: |
      COREOS_PUBLIC_IPV4={{ inventory_hostname }}
      #COREOS_PRIVATE_IPV4=172.17.8.101
      ETCDCTL_CERT_FILE=/etc/ssl/etcd/key.crt
      ETCDCTL_CA_FILE=/etc/ssl/etcd/ca.crt
      ETCDCTL_PEERS=https://{{ inventory_hostname }}:4001
      ETCDCTL_KEY_FILE=/etc/ssl/etcd/key.key
      LOCKSMITHCTL_ETCD_CERTFILE=/etc/ssl/etcd/key.crt
      LOCKSMITHCTL_ETCD_CAFILE=/etc/ssl/etcd/ca.crt
      LOCKSMITHCTL_ENDPOINT=https://{{ inventory_hostname }}:4001
      LOCKSMITHCTL_ETCD_KEYFILE=/etc/ssl/etcd/key.key

  - path: /etc/flannel/options.env
    permissions: 0774
    owner: root
    content: |
      FLANNELD_ETCD_ENDPOINTS=https://{{ inventory_hostname }}:4001
      FLANNELD_ETCD_KEYFILE=/etc/ssl/etcd/key.key
      FLANNELD_ETCD_CERTFILE=/etc/ssl/etcd/key.crt
      FLANNELD_ETCD_CAFILE=/etc/ssl/etcd/ca.crt
      FLANNELD_IFACE={{ inventory_hostname }}

  - path: /srv/tinc_conf_updater.sh
    permissions: 0774
    owner: root
    content: |
      #!/bin/sh
      export DOCKER_HOST=unix:///var/run/early-docker.sock
      . /etc/tinc-env
      host=${ETCD_WATCH_KEY/\/services\/tinc\//}
      #echo "host is $host"
      #echo "$ETCD_WATCH_KEY\" key was updated to \"$ETCD_WATCH_VALUE\" value by \"$ETCD_WATCH_ACTION\" action"
      if [ $TINC_HOSTNAME != $host ]; then
        if [ "$ETCD_WATCH_ACTION" = "set" ]; then
          echo "configuring new tinc host $host"
          current_value="";
          if [ -f /srv/tinc/hosts/$host ]; then
            current_value="$( cat /srv/tinc/hosts/$host )"
          fi
          if [ "$ETCD_WATCH_VALUE" != "\"$current_value\"" ]; then
            docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add ConnectTo = $host
            #etcdctl get /services/tinc/$host | sed -e 's/\"//g' > /srv/tinc/hosts/$host
            echo  "$ETCD_WATCH_VALUE" | sed -e 's/\"//g' > /srv/tinc/hosts/$host
            docker exec tinc /usr/sbin/tinc reload
            echo "done"
          else
           echo "old value = new value; nothing to do"
          fi
        fi
        if [ "$ETCD_WATCH_ACTION" = "delete" ] || [ "$ETCD_WATCH_ACTION" = "expire" ]; then
          echo "removing tinc host $host"
          docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc del ConnectTo = $host
          rm -f /srv/tinc/hosts/$host
          docker exec tinc /usr/sbin/tinc reload
          echo "done"
        fi
      fi
  - path: /etc/ssl/etcd/ca.crt
    permissions: 0644
    content: |
      {{ etcd_ca_certificate|indent(width=6) }}
  
  - path: /etc/ssl/etcd/key.crt
    permissions: 0644
    content: |
      {{ etcd_cert|indent(width=6) }}

  - path: /etc/ssl/etcd/key.key
    permissions: 0644
    content: |
      {{ etcd_key|indent(width=6) }}
  
  - path: /etc/ceph/ceph.conf
    content: {{ ceph_conf.content }}
    encoding: base64

  - path: /etc/ceph/ceph.client.admin.keyring
    content: {{ ceph_admin_keyring.content }}
    encoding: base64

  - path: /etc/ceph/ceph.mon.keyring
    content: {{ ceph_mon_keyring.content }}
    encoding: base64

  - path: /etc/kube_apiserver_haproxy.cfg
    content: |
      global
          daemon
          maxconn 256
      defaults
          mode tcp
          default-server inter 1s fall 2
          timeout connect 5000ms
          timeout client 50000ms
          timeout server 50000ms
      backend apiserver_backend
          mode tcp
          option ssl-hello-chk
          balance source
          hash-type consistent
{% for apiserver in groups['kubernetes-master'] %}
          server kube{{ loop.index }} {{ apiserver }}:6443 check
{% endfor %}
      frontend apiserver_frontend
          mode tcp
          bind *:8080
          default_backend apiserver_backend

coreos:
  #etcd:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new
    # WARNING: replace each time you 'vagrant destroy'
    #discovery: https://discovery.etcd.io/4dbce9b90646e13c17bd298cffc0ed99
    #addr: {{ inventory_hostname }}:4001
    #peer-addr: {{ inventory_hostname }}:7001
  update:
    reboot-strategy: off
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    #discovery: "{# etcd_discovery_url #}"
    name: {{ coreos_hostname }}
    initial-cluster: "{% for host in groups['etcd-node'] %}{{ hostvars[host]['coreos_hostname'] }}=https://{{host}}:2380{%if not loop.last %},{% endif %}{% endfor %}"
    #initial_cluster_state: NEW
    # multi-region and multi-cloud deployments need to use $public_ipv4
    {% if inventory_hostname in groups['etcd-node'] %}

    advertise-client-urls: "https://{{ inventory_hostname }}:2379"
    initial-advertise-peer-urls: "https://{{ inventory_hostname }}:2380"
    listen-peer-urls: "https://{{ inventory_hostname }}:2380,https://{{ inventory_hostname }}:7001"
    {% endif %}

    {% if inventory_hostname in groups['etcd-proxy'] %}

    proxy: on
    {% endif %}
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn't depend on them
    listen-client-urls: "https://0.0.0.0:2379,https://0.0.0.0:4001"

  fleet:
    public-ip: {{ inventory_hostname }}
    #  metadata: role={# coreos_role #}
    etcd_cafile: /etc/ssl/etcd/ca.crt
    etcd_certfile: /etc/ssl/etcd/key.crt
    etcd_keyfile: /etc/ssl/etcd/key.key
    etcd_servers: https://{{ inventory_hostname }}:2379
  locksmith:
    endpoint: https://{{ inventory_hostname }}:2379
    etcd_cafile: /etc/ssl/etcd/ca.crt
    etcd_certfile: /etc/ssl/etcd/key.crt
    etcd_keyfile: /etc/ssl/etcd/key.key
  flannel:
    interface: {{ inventory_hostname }}
  units:
    - name: locksmithd.service
      command: stop
    - name: update-window.service
      runtime: true
      content: |
        [Unit]
        Description=Reboot if an update has been downloaded

        [Service]
        EnvironmentFile=/etc/environment
        ExecStart=/opt/bin/update-window.sh 
    - name: update-window.timer
      runtime: true
      command: start
      content: |
        [Unit]
        Description=Reboot timer

        [Timer]
        OnCalendar=*:0/5

    - name: etcd2.service
      command: start
      drop-ins:
        - name: 50-network-wait.conf
          content: |
            [Unit]
            Requires=systemd-networkd-wait-online.service
            After=systemd-networkd-wait-online.service
        - name: 30-certificates.conf
          content: |
            [Service]
            # Client Env Vars
            Environment=ETCD_CA_FILE=/etc/ssl/etcd/ca.crt
            Environment=ETCD_CERT_FILE=/etc/ssl/etcd/key.crt
            Environment=ETCD_KEY_FILE=/etc/ssl/etcd/key.key
            # Peer Env Vars
            Environment=ETCD_PEER_CA_FILE=/etc/ssl/etcd/ca.crt
            Environment=ETCD_PEER_CERT_FILE=/etc/ssl/etcd/key.crt
            Environment=ETCD_PEER_KEY_FILE=/etc/ssl/etcd/key.key

    - name: update_ca_certificates.service
      command: start
      content: |
        [Unit]
        Before=early-docker.service
        [Service]
        ExecStart=/usr/sbin/update-ca-certificates
        RemainAfterExit=yes
        Type=oneshot


    - name: docker.service
      command: start
      drop-ins: 
        - name: 50-wait-for-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            Wants=flanneld.service
            After=flanneld.service

    - name: 00-wired.network
      runtime: true
      content: |
        [Match]
        Name=en*
        [Network]
        DHCP=ipv4
        DNS=127.0.0.1
        Domains=
{% for ip in hetzner_failover_ips %}
        Address={{ ip }}
{% endfor %}


    #To use etcd2, comment out the above service and uncomment these
    # Note: this requires a release that contains etcd2
    #- name: etcd2.service
    #  command: start
{% if inventory_hostname in groups['kubernetes-master'] %}
    # systemd services master only
    - name: kube-apiserver.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes API Server
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=setup-network-environment.service etcd2.service
        After=setup-network-environment.service etcd2.service kubernetes-networking.target
        Before=kube-kubelet.service
        [Service]
        Environment=KUBELET_VERSION={{ hyperkube_aci_tag }}
        Environment=KUBELET_ACI={{ hyperkube_aci }}
        Environment=EXEC=apiserver
        ExecStart=/opt/bin/hyperkube-wrapper \
        #--service-account-key-file=/opt/bin/kube-serviceaccount.key \
        --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \
        --client-ca-file=/etc/kubernetes/ssl/ca.pem \
        --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem \
        --service-account-lookup=false \
        --admission-control=NamespaceLifecycle,NamespaceAutoProvision,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota \
        --runtime-config=api/v1 \
        --allow-privileged=true \
        --insecure-bind-address=127.0.0.1 \
        --bind-address={{ inventory_hostname }} \
        --insecure-port=8080 \
        #--etcd-config=/etc/etcd-client.config.json \
        --etcd-cafile=/etc/ssl/etcd/ca.crt \
        --etcd-certfile=/etc/ssl/etcd/key.crt \
        --etcd-keyfile=/etc/ssl/etcd/key.key \
        --etcd-servers=https://{{ inventory_hostname }}:2379 \
        --kubelet-https=true \
        --secure-port=6443 \
        --runtime-config=extensions/v1beta1/daemonsets=true \
        --service-cluster-ip-range={{ k8s_service_ip_range }} \
        #--token-auth-file=/srv/kubernetes/known_tokens.csv \
        #--basic-auth-file=/srv/kubernetes/basic_auth.csv \
        #--etcd-servers=http://127.0.0.1:2379 \
        --logtostderr=true
        Restart=always
        RestartSec=10

    - name: kube-controller-manager.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Controller Manager
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=kube-apiserver.service
        After=kube-apiserver.service
        Before=kube-kubelet.service
        [Service]
        Environment=KUBELET_VERSION={{ hyperkube_aci_tag }}
        Environment=KUBELET_ACI={{ hyperkube_aci }}
        Environment=EXEC=controller-manager
        ExecStart=/opt/bin/hyperkube-wrapper \
        --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \
        --root-ca-file=/etc/kubernetes/ssl/ca.pem \
        --master=127.0.0.1:8080 \
        --leader-elect=true \
        --leader-elect-lease-duration=15s \
        --leader-elect-renew-deadline=10s \
        --leader-elect-retry-period=2s \
        --logtostderr=true
        Restart=always
        RestartSec=10
    - name: kube-scheduler.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Scheduler
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=kube-apiserver.service
        After=kube-apiserver.service
        Before=kube-kubelet.service
        [Service]
        Environment=KUBELET_VERSION={{ hyperkube_aci_tag }}
        Environment=KUBELET_ACI={{ hyperkube_aci }}
        Environment=EXEC=scheduler
        ExecStart=/opt/bin/hyperkube-wrapper \
        --master=127.0.0.1:8080 \
        --leader-elect=true \
        --leader-elect-lease-duration=15s \
        --leader-elect-renew-deadline=10s \
        --leader-elect-retry-period=2s
        Restart=always
        RestartSec=10

{% endif %}
{% if inventory_hostname in groups['kubernetes-master'] or inventory_hostname in groups['kubernetes-node'] %}
    # kubernetes network service
    - name: kubernetes-networking.target
      command: start
      content: |
        [Unit]
        Description=services required for proper container networking
    - name: setup-network-environment.service
      command: start
      content: |
        [Unit]
        Description=Setup Network Environment
        Documentation=https://github.com/kelseyhightower/setup-network-environment
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
        ExecStartPre=/usr/bin/curl -L -o /opt/bin/setup-network-environment -z /opt/bin/setup-network-environment https://github.com/kelseyhightower/setup-network-environment/releases/download/v1.0.0/setup-network-environment
        ExecStartPre=/usr/bin/chmod +x /opt/bin/setup-network-environment
        ExecStart=/opt/bin/setup-network-environment
        RemainAfterExit=yes
        Type=oneshot
    - name: setup-subnet-routes.service
      command: start
      content: |
        [Unit]
        Description=Setup Hetzner Subnet Routes
        Documentation=https://github.com/kelseyhightower/setup-network-environment
        Requires=docker.service
        After=docker.service
        [Service]
        ExecStartPre=/usr/bin/docker pull quay.io/cornelius/hetner-netconf
        ExecStart=/usr/bin/docker run --cap-add NET_ADMIN --net=host quay.io/cornelius/hetner-netconf
        RemainAfterExit=yes
        Type=oneshot
    - name: kube-proxy.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=docker.service
        After=docker.service
        Before=kubernetes-networking.target

        [Install]
        WantedBy=kubernetes-networking.target

        [Service]
        TimeoutStartSec=5m
        Environment=KUBELET_VERSION={{ hyperkube_aci_tag }}
        Environment=KUBELET_ACI={{ hyperkube_aci }}
        ExecStartPre=/usr/bin/docker pull {{ hyperkube_aci }}:{{ hyperkube_aci_tag }}
        ExecStartPre=-/usr/bin/docker rm -f kube-proxy
        ExecStart=/usr/bin/docker run \
                   -v  /etc/kubernetes/:/etc/kubernetes/ \
                   --name=kube-proxy \
                   --net=host --privileged=true \
                   {{ hyperkube_aci }}:{{ hyperkube_aci_tag }} \
                   /hyperkube proxy \
                   --master=https://localhost:8888 \
                   --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
                   --logtostderr=true \
                   --proxy-mode=iptables
        ExecStop=/usr/bin/docker rm -f kube-proxy
        Restart=always
        RestartSec=10

    - name: kube-kubelet.service
      command: start
      content: |
        [Unit]
        After=kubernetes-networking.target docker.service
        Requires=kubernetes-networking.target docker.service

        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/docker pull ceph/base
        TimeoutStopSec=60s

        Environment=KUBELET_VERSION={{ hyperkube_aci_tag }}
        Environment=KUBELET_ACI={{ hyperkube_aci }}
        Environment="RKT_OPTS=--volume=ceph,kind=host,source=/etc/ceph/ --mount volume=ceph,target=/etc/ceph/ --volume=modules,kind=host,source=/lib/modules/ --mount volume=modules,target=/lib/modules/ --volume resolv-conf,kind=host,source=/etc/resolv.conf --mount volume=resolv-conf,target=/etc/resolv.conf --volume var-log-containers,kind=host,source=/var/log/containers/ --mount volume=var-log-containers,target=/var/log/containers/"
        #--volume sys,kind=host,source=/sys --mount volume=sys,target=/sys --volume dev,kind=host,source=/dev --mount volume=dev,target=/dev"
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=https://127.0.0.1:8888 \
          --allow-privileged=true \
          --config=/etc/kubernetes/manifests \
          --cluster-dns={{ k8s_dns_service_ip }} \
          --cluster-domain={{ k8s_dns_domain }} \
          --hostname-override={{ inventory_hostname }} \
          --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
          --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
          --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem \
          --register-node=true \
          --register-schedulable=true \
          --node-status-update-frequency=2s
        ExecStartPost=-/opt/bin/uncordon-node
        ExecStop=/opt/bin/drain-node

        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target
    - name: kube-apiserver-haproxy.service
      enable: true
      command: start
      content: |
         [Unit]
         Description=Ha Proxy for kubernetes api server
         After=docker.service
         Before=kubernetes-networking.target

         [Service]
         TimeoutStartSec=5m
         ExecStartPre=-/usr/bin/docker kill haproxy
         ExecStartPre=-/usr/bin/docker rm -f haproxy
         ExecStartPre=/usr/bin/docker pull haproxy:alpine
         ExecStart=/usr/bin/docker run --rm \
           --name haproxy \
           -v /etc/kube_apiserver_haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg \
           -p 127.0.0.1:8888:8080 \
           haproxy:alpine 
         ExecStop=-/usr/bin/docker stop haproxy
         ExecStop=-/usr/bin/docker rm haproxy
         Restart=always
         RestartSec=10

         [Install]
         WantedBy=kubernetes-networking.target

{% endif %}

    - name: early-docker.service
      command: start
      enable: true
    - name: tinc-env.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Tinc Service
        After=etcd.service etcd2.service early-docker.service flanneld.service
        Before=early-docker.target fleet.service
        
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo \"TINC_HOSTNAME=`hostname | sed -e 's/-/_/g'`\" > /etc/tinc-env"
    - name: flannel-wait.service
      command: start
      enable: true
      content: |

        [Unit]
        Description=Wait For Flannel
        Requires=flanneld.service
        After=etcd.service etcd2.service early-docker.service flanneld.service
        Before=early-docker.target

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo \"TINC_HOSTNAME=`hostname | sed -e 's/-/_/g'`\" > /etc/tinc-env"
        ExecStartPre=/bin/sh -c "while [ ! -f /run/flannel/subnet.env ] ; do sleep 1; done"

    - name: etcd-waiter.service
      command: start
      content: |
        [Unit]
        Description=etcd waiter
        Wants=network-online.target
        Wants=etcd2.service
        After=etcd2.service
        After=network-online.target
        Before=flanneld.service
        Before=setup-network-environment.service

        [Service]
        ExecStartPre=/usr/bin/chmod +x /opt/bin/waiter.sh
        ExecStart=/usr/bin/bash /opt/bin/waiter.sh
        RemainAfterExit=true
        Type=oneshot

    - name: tinc-conf.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Tinc Configuration Service
        After=etcd.service etcd2.service early-docker.service flanneld.service
        Before=early-docker.target fleet.service

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo \"TINC_HOSTNAME=`hostname | sed -e 's/-/_/g'`\" > /etc/tinc-env"
    - name: dnsmask.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=dnsmask service
        Requires=docker.service
        Before=kubernetes-networking.target

        [Install]
        WantedBy=kubernetes-networking.target


        [Service]
        ExecStartPre=/usr/bin/docker pull quay.io/coreos/dnsmasq
        ExecStartPre=-/usr/bin/docker rm -f dnsmask
        ExecStart=/usr/bin/docker run  --name dnsmask --net=host --cap-add=NET_ADMIN quay.io/coreos/dnsmasq -d -q  \
          --listen-address=127.0.0.1 \
          --server=/cluster.local/{{ k8s_dns_service_ip }} \
          --rev-server={{ k8s_service_ip_range }},{{ k8s_dns_service_ip }} \
          --rev-server=10.1.0.0/16,{{ k8s_dns_service_ip }}

        #ExecStop=/usr/bin/sleep 50
        ExecStopPost=/usr/bin/docker rm -f dnsmask
        Restart=always
        RestartSec=10
        TimeoutStartSec=5m


    - name: tinc.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Tinc VPN Service
        Requires=flannel-wait.service
        After=early-docker.service flanneld.service tinc-env.service flannel-wait.service 
        Before=kubernetes-networking.target tinc-config-updater.service

        [Install]
        WantedBy=kubernetes-networking.target

        [Service]
        Environment="DOCKER_HOST=unix:///var/run/early-docker.sock"
        EnvironmentFile=/etc/tinc-env

        EnvironmentFile=/etc/environment


        ExecStartPre=/usr/bin/docker pull jenserat/tinc
        ExecStartPre=/usr/bin/rm -rf /srv/tinc
        ExecStartPre=/usr/bin/mkdir -p /srv/tinc
        ExecStartPre=/bin/sh -c "/usr/bin/docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc init $TINC_HOSTNAME"
        ExecStartPre=/bin/sh -c "/usr/bin/docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add Address = $COREOS_PUBLIC_IPV4"
        TimeoutStartSec=5m
        EnvironmentFile=/run/flannel/subnet.env
        ExecStartPre=/bin/sh -c "/usr/bin/docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add Subnet = `echo $FLANNEL_SUBNET | sed -e 's/1\\/24/0\\/24/'`"
        ExecStartPre=/bin/sh -c "/usr/bin/docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add Mode = switch"
        ExecStartPre=/bin/sh -c "/usr/bin/docker run --rm --volume /srv/tinc:/etc/tinc  jenserat/tinc add DeviceType = tap"
        ExecStartPre=-/usr/bin/docker rm -f tinc 
        ExecStartPre=/usr/bin/docker run --name tinc  -d --volume /srv/tinc:/etc/tinc --net=host --device=/dev/net/tun --cap-add NET_ADMIN jenserat/tinc start  -D

        ExecStart=/bin/sh -c "while true; do etcdctl set /services/tinc/$TINC_HOSTNAME  \"\\\"` cat /srv/tinc/hosts/$TINC_HOSTNAME `\"\\\" --ttl 60;sleep 45;done"

        ExecStop=/usr/bin/docker rm -f tinc
        ExecStopPost=/bin/sh -c  "etcdctl rm /services/tinc/$TINC_HOSTNAME"

    - name: tinc-config-updater.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Countinously update tinc configuration after ectd changes
        After=tinc.service
        Before=kubernetes-networking.target
        Restart=always

        [Install]
        WantedBy=kubernetes-networking.target
        
        [Service]
        Restart=always
        RestartSec=10
        EnvironmentFile=/etc/environment
        ExecStartPre=/srv/tinc_initial_config.sh
        ExecStart=/usr/bin/etcdctl exec-watch --recursive /services/tinc -- /srv/tinc_conf_updater.sh


    - name: flanneld.service
      command: start
      enable: true
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service etcd-waiter.service
            Before=docker.service
            [Service]
            EnvironmentFile=/etc/environment
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16", "Backend": { "Type": "alloc"} }'
        - name: 40-symlink.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env


    - name: docker-bridge.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Configure Docker Bridge
        Requires=docker.service
        #After=docker.socket
        [Service]
        Type=oneshot
        #ExecStartPre=-/bin/sh -c "route del -net 10.1.0.0 netmask 255.255.0.0 dev tap0"
        ExecStartPre=/bin/sh -c "while ! ifconfig -s | grep -q tap0 ; do  sleep 1; done"
        ExecStartPre=/bin/sh -c "while ! ifconfig -s | grep -q docker0 ; do  sleep 1; done"
        ExecStartPre=/bin/sh -c "route add -net 10.1.0.0 netmask 255.255.0.0 dev docker0"
        #ExecStartPre=-/bin/sh -c "brctl delif docker0 tap0"
        ExecStart=/bin/sh -c "brctl addif docker0 tap0"

    - name: fleet.service
      command: start
    - name: clenup-at-shutdown.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Cleanup directories populated by cloudconfig to avoid configuration drift
        Before=network.target
        RequiredBy=kube-kubelet.service

        [Install]
        WantedBy=kubernetes-networking.target


        [Service]
        RemainAfterExit=yes
        ExecStart=/bin/true
        ExecStopPost=/opt/bin/cleanup
        Type=oneshot
